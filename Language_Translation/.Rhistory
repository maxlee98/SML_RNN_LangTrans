predict_output
# str(model_RNN)
# single <- tail(tensor_x)[1, ,1]
# single <- matrix(single, ncol=1)
# single <- array(single, c(dim(single)[1], dim(single)[2], 1))
# single
# matrix(tensor_x[50, ,], nrow=1)
# convert2tensor(tensor_x[1:50, ,])
# tensor_x[8509, ,]
# model_RNN$input
predict_output <- model_RNN %>% predict(convert2tensor(tensor_x[1:5, ,]))
# length(tensor_x)
dim(predict_output)
# head(predict_output)
length(y_tk[[1]])
logits_to_text(predict_output, y_tk)
dim(predict_output)
local <- list()
for(i in 1:2){
local[1] <- argmax(predict_output[i, ,], FALSE)
}
print(local)
predict_output
# str(model_RNN)
# single <- tail(tensor_x)[1, ,1]
# single <- matrix(single, ncol=1)
# single <- array(single, c(dim(single)[1], dim(single)[2], 1))
# single
# matrix(tensor_x[50, ,], nrow=1)
# convert2tensor(tensor_x[1:50, ,])
# tensor_x[8509, ,]
# model_RNN$input
predict_output <- model_RNN %>% predict(convert2tensor(tensor_x[1:5, ,]))
# length(tensor_x)
dim(predict_output)
# head(predict_output)
length(y_tk[[1]])
logits_to_text(predict_output, y_tk)
dim(predict_output)
local <- list()
for(i in 1:2){
local[1] <- argmax(predict_output[i, ,], FALSE)
}
print(local)
# predict_output
logits_to_text(predict_output, y_tk)
# str(model_RNN)
# single <- tail(tensor_x)[1, ,1]
# single <- matrix(single, ncol=1)
# single <- array(single, c(dim(single)[1], dim(single)[2], 1))
# single
# matrix(tensor_x[50, ,], nrow=1)
# convert2tensor(tensor_x[1:50, ,])
# tensor_x[8509, ,]
# model_RNN$input
predict_output <- model_RNN %>% predict(tensor_x[1, ,]))
# str(model_RNN)
# single <- tail(tensor_x)[1, ,1]
# single <- matrix(single, ncol=1)
# single <- array(single, c(dim(single)[1], dim(single)[2], 1))
# single
# matrix(tensor_x[50, ,], nrow=1)
# convert2tensor(tensor_x[1:50, ,])
# tensor_x[8509, ,]
# model_RNN$input
predict_output <- model_RNN %>% predict(matrix(tensor_x[1, ,], nrow=1))
logits_to_text(predict_output, y_tk)
argmax(predict_output, FALSE)
predict_output <- argmax(predict_output, FALSE)
logits_to_text(predict_output, y_tk)
subset_train_x[1]
logits_to_text <- function(logits, tokenizer){
tokenizer_words <- attributes(tokenizer[[1]])$names
text <- c()
logits <- logits - 1
for(i in logits){
if(i == 0){
text <- c(text, "<PAD>")
}else{
text <- c(text, tokenizer_words[i])
}
}
return(text)
}
# Testing to convert the first row back to text
# preprocess_x[[1]]
logits_to_text(preprocess_x[[1]][1, ], x_tk)
# str(model_RNN)
# single <- tail(tensor_x)[1, ,1]
# single <- matrix(single, ncol=1)
# single <- array(single, c(dim(single)[1], dim(single)[2], 1))
# single
# matrix(tensor_x[50, ,], nrow=1)
# convert2tensor(tensor_x[1:50, ,])
# tensor_x[8509, ,]
# model_RNN$input
predict_output <- model_RNN %>% predict(matrix(tensor_x[5, ,], nrow=1))
subset_train_x[1]
predict_output <- argmax(predict_output, FALSE)
# length(tensor_x)
dim(predict_output)
# head(predict_output)
length(y_tk[[1]])
logits_to_text(predict_output, y_tk)
dim(predict_output)
local <- list()
for(i in 1:2){
local[1] <- argmax(predict_output[i, ,], FALSE)
}
subset_train_y[2]
# str(model_RNN)
# single <- tail(tensor_x)[1, ,1]
# single <- matrix(single, ncol=1)
# single <- array(single, c(dim(single)[1], dim(single)[2], 1))
# single
# matrix(tensor_x[50, ,], nrow=1)
# convert2tensor(tensor_x[1:50, ,])
# tensor_x[8509, ,]
# model_RNN$input
predict_output <- model_RNN %>% predict(matrix(tensor_x[5, ,], nrow=1))
subset_train_x[1]
subset_train_y[1]
predict_output <- argmax(predict_output, FALSE)
# length(tensor_x)
dim(predict_output)
# head(predict_output)
length(y_tk[[1]])
logits_to_text(predict_output, y_tk)
dim(predict_output)
local <- list()
for(i in 1:2){
local[1] <- argmax(predict_output[i, ,], FALSE)
}
lower_bound_words <- 5; upper_bound_words <- 10
subset_train <- subset(train,
train$english_length >= lower_bound_words & train$english_length <= upper_bound_words
& train$other_length >= lower_bound_words & train$other_length <= upper_bound_words
)
## Checking for the number of rows within the new subsetted dataframe for testing purposes.
head(subset_train)
tail(subset_train)
nrow(subset_train)
library(keras)
library(tensorflow)
library(tokenizers)
library(dplyr)
library(png)
library(reticulate)
library(abind)
library(ramify)
language <- "Indonesian"
language_code <- "ind"
file_name <- paste0("translation_", language_code, ".csv")
train <- read.csv(file_name, encoding="UTF-8", stringsAsFactors=FALSE)
colnames(train) <- c("English", language)
train
tokenize <- function(x){
tokenizer <- text_tokenizer(num_words = 10000)
fit_text_tokenizer(tokenizer, x)
sequences <- texts_to_sequences(tokenizer, x)
return(c(sequences, tokenizer))
}
pad <- function(x, length=NULL){
return(pad_sequences(x, maxlen = length, padding = 'post'))
}
sentences_length_vec <- function(word_list){
output <- tokenize(word_list)
sentence_length <- c()
for(i in 1:length(word_list)){
sentence_length[i] <- length(output[[i]])
}
sentence_length
}
english_sentence_length <- sentences_length_vec(list(train[, 1])[[1]])
other_sentence_length <- sentences_length_vec(list(train[, 2])[[1]])
## Adding each sentence length to the dataframe `train`
train$english_length <- english_sentence_length
train$other_length <- other_sentence_length
tail(train)
lower_bound_words <- 5; upper_bound_words <- 10
subset_train <- subset(train,
train$english_length >= lower_bound_words & train$english_length <= upper_bound_words
& train$other_length >= lower_bound_words & train$other_length <= upper_bound_words
)
## Checking for the number of rows within the new subsetted dataframe for testing purposes.
head(subset_train)
tail(subset_train)
nrow(subset_train)
text_sentences = c('The quick brown fox jumps over the lazy dog .',
'By Jove , my quick study of lexicography won a prize .',
'This is a short sentence .')
token_index <- length(text_sentences) + 1
output <- tokenize(text_sentences)
text_tokenized <- output[1:length(text_sentences)]
# print(output)
# Finding out the integer allocation to each word
tk <- output[[token_index]]$word_index
# print(tk)
# print(length(tk))
# print(table(tk))
for(i in 1:length(text_sentences)){
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", text_sentences[i]))
print(paste0("Output: ", list(text_tokenized[[i]])))
}
# padded_text <- pad(text_tokenized)
# for(i in 1:length(text_sentences)){
#   print(paste0("Sequence in Text ", i, ":"))
#   print(paste0("Input: ", text_sentences[i]))
#   print(paste0("Output: ", list(text_tokenized[[i]])))
#   print(paste0("Output (Padded): ", list(padded_text[i,])))
# }
n <- nrow(subset_train)
word_list <- list(subset_train[, 1])[[1]][1:n]
# word_list
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
}
n <- nrow(subset_train)
word_list <- list(subset_train[, 2])[[1]][1:n]
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
}
preprocess_text <- function(x, y){
output_x <- tokenize(x)
output_y <- tokenize(y)
preprocess_x <- output_x[1:length(x)]; x_tk <- output_x[[length(x) + 1]]$word_index
preprocess_y <- output_y[1:length(y)]; y_tk <- output_y[[length(y) + 1]]$word_index
# print(preprocess_x)
preprocess_x <- pad(preprocess_x)
preprocess_y <- pad(preprocess_y)
# print(preprocess_x)
# Converting from a 2D matrix to a 3D tensor
# preprocess_x <- array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# preprocess_y <- array(preprocess_y[[1]], c(dim(preprocess_y[[1]])[1], dim(preprocess_y[[1]])[2], 1))
return(list(preprocess_x, preprocess_y, x_tk, y_tk))
}
n <- nrow(subset_train) #1000
subset_train_x <- list(subset_train[, 1])[[1]][1:n]
subset_train_y <- list(subset_train[, 2])[[1]][1:n]
# print(subset_train_x)
process_output <- preprocess_text(subset_train_x, subset_train_y)
# print(process_output[4],)
preprocess_x <- process_output[1]; preprocess_y <- process_output[2]; x_tk <- process_output[3]; y_tk <- process_output[4]
# print(preprocess_x[[1]])
# print(preprocess_y[[1]])
# Conversion back to list of words from tokenized word list
# attributes(x_tk[[1]])$names
# length(y_tk[[1]])
col_x <- dim(preprocess_x[[1]])[2]
col_y <- dim(preprocess_y[[1]])[2]
if(col_x >= col_y){
max_col <- col_x
}else{
max_col <- col_y
}
tmp_x <- pad(preprocess_x[[1]], max_col)
tmp_y <- pad(preprocess_y[[1]], max_col)
row <- 5
head(tmp_x)
subset_train_x[row]
# x_tk[[1]]$jump
convert2tensor <- function(preprocess_data){
preprocess_data <- array(preprocess_data, c(dim(preprocess_data)[1], dim(preprocess_data)[2], 1))
return(preprocess_data)
}
# array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# dim(array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1)))[2:3]
tensor_x <- convert2tensor(tmp_x)
dim(tensor_x)
tensor_x[1, , ]
tensor_y <- convert2tensor(tmp_y)
# tensor_y
logits_to_text <- function(logits, tokenizer){
tokenizer_words <- attributes(tokenizer[[1]])$names
text <- c()
logits <- logits - 1
for(i in logits){
if(i == 0){
text <- c(text, "<PAD>")
}else{
text <- c(text, tokenizer_words[i])
}
}
return(text)
}
# Testing to convert the first row back to text
# preprocess_x[[1]]
# logits_to_text(preprocess_x[[1]][1, ], x_tk)
# dim(tensor_y)
model_RNN <-  keras_model_sequential()
model_RNN %>%
layer_gru(units = 256, input_shape = dim(tensor_x)[2:3], return_sequences = TRUE) %>%
time_distributed(layer_dense(units = 1024, activation = 'relu'))%>%
layer_dropout(rate = 0.5) %>%
time_distributed(layer_dense(units = length(y_tk[[1]]) + 1, activation = 'softmax'))
model_RNN %>% summary()
model_RNN %>% compile(
loss      = 'sparse_categorical_crossentropy',
# optimizer = optimizer_rmsprop(),
optimizer = optimizer_adam(learning_rate = 0.005),
metrics=c('accuracy')
)
history = model_RNN %>% fit(
x = tensor_x, y = tensor_y,
epochs           = 10,
batch_size = 20,
validation_split = 0.2,
)
plot(history)
# tail(attributes(y_tk[[1]])$names)
# y_tk[[1]]$'<PAD>' <- 0
# tail(y_tk[[1]])
# y_tk[[1]]$"<PAD>"
# x_tk[[1]]
# str(model_RNN)
# single <- tail(tensor_x)[1, ,1]
# single <- matrix(single, ncol=1)
# single <- array(single, c(dim(single)[1], dim(single)[2], 1))
# single
# matrix(tensor_x[50, ,], nrow=1)
# convert2tensor(tensor_x[1:50, ,])
# tensor_x[8509, ,]
# model_RNN$input
predict_output <- model_RNN %>% predict(matrix(tensor_x[5, ,], nrow=1))
subset_train_x[1]
subset_train_y[1]
predict_output <- argmax(predict_output, FALSE)
# length(tensor_x)
dim(predict_output)
# head(predict_output)
length(y_tk[[1]])
logits_to_text(predict_output, y_tk)
dim(predict_output)
local <- list()
for(i in 1:2){
local[1] <- argmax(predict_output[i, ,], FALSE)
}
calculate_sparsity <- function(df_matrix){
zero_count <- 0
total_count <- nrow(df_matrix) * ncol(tmp_x)
for(i in 1:nrow(df_matrix)){
for(j in 1:ncol(df_matrix)){
if(df_matrix[i, j] == 0){
zero_count = zero_count + 1
}
}
}
zero_count/total_count
}
calculate_sparsity(tmp_x)
library(keras)
library(tensorflow)
library(tokenizers)
library(dplyr)
library(png)
library(reticulate)
library(abind)
library(ramify)
language <- "Indonesian"
language_code <- "ind"
file_name <- paste0("translation_", language_code, ".csv")
train <- read.csv(file_name, encoding="UTF-8", stringsAsFactors=FALSE)
colnames(train) <- c("English", language)
train
tokenize <- function(x){
tokenizer <- text_tokenizer(num_words = 10000)
fit_text_tokenizer(tokenizer, x)
sequences <- texts_to_sequences(tokenizer, x)
return(c(sequences, tokenizer))
}
pad <- function(x, length=NULL){
return(pad_sequences(x, maxlen = length, padding = 'post'))
}
sentences_length_vec <- function(word_list){
output <- tokenize(word_list)
sentence_length <- c()
for(i in 1:length(word_list)){
sentence_length[i] <- length(output[[i]])
}
sentence_length
}
english_sentence_length <- sentences_length_vec(list(train[, 1])[[1]])
other_sentence_length <- sentences_length_vec(list(train[, 2])[[1]])
## Adding each sentence length to the dataframe `train`
train$english_length <- english_sentence_length
train$other_length <- other_sentence_length
tail(train)
lower_bound_words <- 8; upper_bound_words <- 10
subset_train <- subset(train,
train$english_length >= lower_bound_words & train$english_length <= upper_bound_words
& train$other_length >= lower_bound_words & train$other_length <= upper_bound_words
)
## Checking for the number of rows within the new subsetted dataframe for testing purposes.
head(subset_train)
tail(subset_train)
nrow(subset_train)
text_sentences = c('The quick brown fox jumps over the lazy dog .',
'By Jove , my quick study of lexicography won a prize .',
'This is a short sentence .')
token_index <- length(text_sentences) + 1
output <- tokenize(text_sentences)
text_tokenized <- output[1:length(text_sentences)]
# print(output)
# Finding out the integer allocation to each word
tk <- output[[token_index]]$word_index
# print(tk)
# print(length(tk))
# print(table(tk))
for(i in 1:length(text_sentences)){
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", text_sentences[i]))
print(paste0("Output: ", list(text_tokenized[[i]])))
}
# padded_text <- pad(text_tokenized)
# for(i in 1:length(text_sentences)){
#   print(paste0("Sequence in Text ", i, ":"))
#   print(paste0("Input: ", text_sentences[i]))
#   print(paste0("Output: ", list(text_tokenized[[i]])))
#   print(paste0("Output (Padded): ", list(padded_text[i,])))
# }
n <- nrow(subset_train)
word_list <- list(subset_train[, 1])[[1]][1:n]
# word_list
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
}
n <- nrow(subset_train)
word_list <- list(subset_train[, 2])[[1]][1:n]
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
}
preprocess_text <- function(x, y){
output_x <- tokenize(x)
output_y <- tokenize(y)
preprocess_x <- output_x[1:length(x)]; x_tk <- output_x[[length(x) + 1]]$word_index
preprocess_y <- output_y[1:length(y)]; y_tk <- output_y[[length(y) + 1]]$word_index
# print(preprocess_x)
preprocess_x <- pad(preprocess_x)
preprocess_y <- pad(preprocess_y)
# print(preprocess_x)
# Converting from a 2D matrix to a 3D tensor
# preprocess_x <- array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# preprocess_y <- array(preprocess_y[[1]], c(dim(preprocess_y[[1]])[1], dim(preprocess_y[[1]])[2], 1))
return(list(preprocess_x, preprocess_y, x_tk, y_tk))
}
n <- nrow(subset_train) #1000
subset_train_x <- list(subset_train[, 1])[[1]][1:n]
subset_train_y <- list(subset_train[, 2])[[1]][1:n]
# print(subset_train_x)
process_output <- preprocess_text(subset_train_x, subset_train_y)
# print(process_output[4],)
preprocess_x <- process_output[1]; preprocess_y <- process_output[2]; x_tk <- process_output[3]; y_tk <- process_output[4]
# print(preprocess_x[[1]])
# print(preprocess_y[[1]])
# Conversion back to list of words from tokenized word list
# attributes(x_tk[[1]])$names
# length(y_tk[[1]])
col_x <- dim(preprocess_x[[1]])[2]
col_y <- dim(preprocess_y[[1]])[2]
if(col_x >= col_y){
max_col <- col_x
}else{
max_col <- col_y
}
tmp_x <- pad(preprocess_x[[1]], max_col)
tmp_y <- pad(preprocess_y[[1]], max_col)
row <- 5
head(tmp_x)
subset_train_x[row]
# x_tk[[1]]$jump
calculate_sparsity <- function(df_matrix){
zero_count <- 0
total_count <- nrow(df_matrix) * ncol(tmp_x)
for(i in 1:nrow(df_matrix)){
for(j in 1:ncol(df_matrix)){
if(df_matrix[i, j] == 0){
zero_count = zero_count + 1
}
}
}
zero_count/total_count
}
calculate_sparsity(tmp_x)
