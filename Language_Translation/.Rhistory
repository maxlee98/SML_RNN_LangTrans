<<<<<<< Updated upstream
# predict_output
predict_output <- argmax(predict_output, FALSE)
# train_x[5]
train_y[5]
logits_to_text(predict_output, y_tk, predict = TRUE)
plot_deepviz(model_RNN)
plot_model(model_RNN)
library(keras)
library(tensorflow)
library(tokenizers)
library(dplyr)
library(png)
library(reticulate)
library(abind)
library(ramify)
library(stringr)
library(deepviz)
language <- "French"
language_code <- "fr"
file_name <- paste0("translation_", language_code, ".csv")
train <- read.csv(file_name, encoding="UTF-8", stringsAsFactors=FALSE)
colnames(train) <- c("English", language)
train
tokenize <- function(x){
tokenizer <- text_tokenizer(num_words = 1000000)
fit_text_tokenizer(tokenizer, x)
sequences <- texts_to_sequences(tokenizer, x)
return(c(sequences, tokenizer))
}
pad <- function(x, length=NULL){
return(pad_sequences(x, maxlen = length, padding = 'post'))
}
text_sentences = c('The quick brown fox jumps over the lazy dog .',
'By Jove , my quick study of lexicography won a prize .',
'This is a short sentence .')
token_index <- length(text_sentences) + 1
output <- tokenize(text_sentences)
text_tokenized <- output[1:length(text_sentences)]
# print(output)
# Finding out the integer allocation to each word
tk <- output[[token_index]]$word_index
# print(tk)
# print(length(tk))
# print(table(tk))
for(i in 1:length(text_sentences)){
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", text_sentences[i]))
print(paste0("Output: ", list(text_tokenized[[i]])))
cat("\n")
}
padded_text <- pad(text_tokenized)
for(i in 1:length(text_sentences)){
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", text_sentences[i]))
print(paste0("Output: ", list(text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(padded_text[i,])))
}
# n <- nrow(subset_train)
n <- 5
word_list <- list(train[, 1])[[1]][1:n]
# word_list
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
# if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
# cat("\n")
}
# n <- nrow(subset_train)
n <- 5
word_list <- list(train[, 2])[[1]][1:n]
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
# if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
}
preprocess_text <- function(x, y){
output_x <- tokenize(x)
output_y <- tokenize(y)
preprocess_x <- output_x[1:length(x)]; x_tk <- output_x[[length(x) + 1]]$word_index
preprocess_y <- output_y[1:length(y)]; y_tk <- output_y[[length(y) + 1]]$word_index
# print(preprocess_x)
preprocess_x <- pad(preprocess_x)
preprocess_y <- pad(preprocess_y)
# print(preprocess_x)
# Converting from a 2D matrix to a 3D tensor
# preprocess_x <- array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# preprocess_y <- array(preprocess_y[[1]], c(dim(preprocess_y[[1]])[1], dim(preprocess_y[[1]])[2], 1))
return(list(preprocess_x, preprocess_y, x_tk, y_tk))
}
train_x <- list(train[, 1])[[1]]
train_y <- list(train[, 2])[[1]]
# print(subset_train_x)
process_output <- preprocess_text(train_x, train_y)
# print(process_output[4],)
preprocess_x <- process_output[1]; preprocess_y <- process_output[2]; x_tk <- process_output[3]; y_tk <- process_output[4]
# print(preprocess_x[[1]])
# print(preprocess_y[[1]])
# Conversion back to list of words from tokenized word list
# attributes(x_tk[[1]])$names
# length(y_tk[[1]])
col_x <- dim(preprocess_x[[1]])[2]
col_y <- dim(preprocess_y[[1]])[2]
if(col_x >= col_y){
max_col <- col_x
}else{
max_col <- col_y
}
tmp_x <- pad(preprocess_x[[1]], max_col)
tmp_y <- pad(preprocess_y[[1]], max_col)
calculate_sparsity <- function(df_matrix){
zero_count <- 0
total_count <- nrow(df_matrix) * ncol(tmp_x)
for(i in 1:nrow(df_matrix)){
for(j in 1:ncol(df_matrix)){
if(df_matrix[i, j] == 0){
zero_count = zero_count + 1
}
}
}
zero_count/total_count
}
print(paste("The Sparsity of the matrix is: ", round(calculate_sparsity(tmp_x)*100, 2), "%"))
convert2tensor <- function(preprocess_data){
preprocess_data <- array(preprocess_data, c(dim(preprocess_data)[1], dim(preprocess_data)[2], 1))
return(preprocess_data)
}
# array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# dim(array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1)))[2:3]
tensor_x <- convert2tensor(tmp_x)
dim(tensor_x)
tensor_x[1, , ]
tensor_y <- convert2tensor(tmp_y)
# tensor_y
logits_to_text <- function(logits, tokenizer, predict=FALSE){
tokenizer_words <- attributes(tokenizer[[1]])$names
text <- c()
if(predict == TRUE){
logits <- logits - 1 ## For prediction conversion only
}
for(i in logits){
if(i == 0){
text <- c(text, "<PAD>")
}else{
text <- c(text, tokenizer_words[i])
}
}
return(text)
}
# Testing to convert the first row back to text
# preprocess_x[[1]][1, ]
# preprocess_x[[1]]
logits_to_text(preprocess_x[[1]][1, ], x_tk)
# dim(tensor_y)
model_RNN <-  keras_model_sequential()
model_RNN %>%
layer_simple_rnn(units = 256, input_shape = dim(tensor_x)[2:3], return_sequences = TRUE) %>%
layer_dense(units = 1024, activation = 'relu')%>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(y_tk[[1]]) + 1, activation = 'softmax')
model_RNN %>% summary()
model_RNN %>% compile(
loss      = 'sparse_categorical_crossentropy',
# optimizer = optimizer_rmsprop(),
optimizer = optimizer_adam(learning_rate = 0.005),
metrics=c('accuracy')
)
plot_model(model_RNN)
history = model_RNN %>% fit(
x = tensor_x, y = tensor_y,
epochs           = 10,
batch_size = 1024,
validation_split = 0.2,
)
plot(history)
predict_output <- model_RNN %>% predict(matrix(tensor_x[5, ,], nrow=1))
# predict_output
predict_output <- argmax(predict_output, FALSE)
# train_x[5]
train_y[5]
logits_to_text(predict_output, y_tk, predict = TRUE)
pred_translation <- function(i){
predict_output <- model_RNN %>% predict(matrix(tensor_x[i, ,], nrow=1))
predict_output <- argmax(predict_output, FALSE)
converted_text <- logits_to_text(predict_output, y_tk, predict = TRUE)
converted_text[converted_text == "<PAD>"] <- ""
converted_text <- trimws(paste(converted_text, collapse = " "))
print(paste("Input sentence:", train_x[i]))
print(paste("Intended Output Sentence:", train_y[i]))
print(paste("Predicted Output Sentence:", converted_text))
}
## `i` represents the index within the training set.
pred_translation(5)
library(keras)
library(tensorflow)
library(tokenizers)
library(dplyr)
# library(png)
library(reticulate)
# library(abind)
library(ramify)
library(stringr)
library(deepviz)
language <- "French"
language_code <- "fr"
file_name <- paste0("translation_", language_code, ".csv")
train <- read.csv(file_name, encoding="UTF-8", stringsAsFactors=FALSE)
colnames(train) <- c("English", language)
train
tokenize <- function(x){
tokenizer <- text_tokenizer(num_words = 1000000)
fit_text_tokenizer(tokenizer, x)
sequences <- texts_to_sequences(tokenizer, x)
return(c(sequences, tokenizer))
}
pad <- function(x, length=NULL){
return(pad_sequences(x, maxlen = length, padding = 'post'))
}
text_sentences = c('The quick brown fox jumps over the lazy dog .',
'By Jove , my quick study of lexicography won a prize .',
'This is a short sentence .')
token_index <- length(text_sentences) + 1
output <- tokenize(text_sentences)
text_tokenized <- output[1:length(text_sentences)]
# print(output)
# Finding out the integer allocation to each word
tk <- output[[token_index]]$word_index
# print(tk)
# print(length(tk))
# print(table(tk))
for(i in 1:length(text_sentences)){
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", text_sentences[i]))
print(paste0("Output: ", list(text_tokenized[[i]])))
cat("\n")
}
padded_text <- pad(text_tokenized)
for(i in 1:length(text_sentences)){
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", text_sentences[i]))
print(paste0("Output: ", list(text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(padded_text[i,])))
}
# n <- nrow(subset_train)
n <- 5
word_list <- list(train[, 1])[[1]][1:n]
=======
>>>>>>> Stashed changes
# word_list
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
# if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
# cat("\n")
}
# n <- nrow(subset_train)
n <- 5
word_list <- list(train[, 2])[[1]][1:n]
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
# if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
}
preprocess_text <- function(x, y){
output_x <- tokenize(x)
output_y <- tokenize(y)
preprocess_x <- output_x[1:length(x)]; x_tk <- output_x[[length(x) + 1]]$word_index
preprocess_y <- output_y[1:length(y)]; y_tk <- output_y[[length(y) + 1]]$word_index
# print(preprocess_x)
preprocess_x <- pad(preprocess_x)
preprocess_y <- pad(preprocess_y)
# print(preprocess_x)
# Converting from a 2D matrix to a 3D tensor
# preprocess_x <- array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# preprocess_y <- array(preprocess_y[[1]], c(dim(preprocess_y[[1]])[1], dim(preprocess_y[[1]])[2], 1))
return(list(preprocess_x, preprocess_y, x_tk, y_tk))
}
train_x <- list(train[, 1])[[1]]
train_y <- list(train[, 2])[[1]]
# print(subset_train_x)
process_output <- preprocess_text(train_x, train_y)
# print(process_output[4],)
preprocess_x <- process_output[1]; preprocess_y <- process_output[2]; x_tk <- process_output[3]; y_tk <- process_output[4]
# print(preprocess_x[[1]])
# print(preprocess_y[[1]])
# Conversion back to list of words from tokenized word list
# attributes(x_tk[[1]])$names
# length(y_tk[[1]])
col_x <- dim(preprocess_x[[1]])[2]
col_y <- dim(preprocess_y[[1]])[2]
if(col_x >= col_y){
max_col <- col_x
}else{
max_col <- col_y
}
tmp_x <- pad(preprocess_x[[1]], max_col)
tmp_y <- pad(preprocess_y[[1]], max_col)
calculate_sparsity <- function(df_matrix){
zero_count <- 0
total_count <- nrow(df_matrix) * ncol(tmp_x)
for(i in 1:nrow(df_matrix)){
for(j in 1:ncol(df_matrix)){
if(df_matrix[i, j] == 0){
zero_count = zero_count + 1
}
}
}
zero_count/total_count
}
<<<<<<< Updated upstream
print(paste("The Sparsity of the matrix is: ", round(calculate_sparsity(tmp_x)*100, 2), "%"))
convert2tensor <- function(preprocess_data){
preprocess_data <- array(preprocess_data, c(dim(preprocess_data)[1], dim(preprocess_data)[2], 1))
return(preprocess_data)
}
# array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# dim(array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1)))[2:3]
tensor_x <- convert2tensor(tmp_x)
dim(tensor_x)
tensor_x[1, , ]
tensor_y <- convert2tensor(tmp_y)
# tensor_y
logits_to_text <- function(logits, tokenizer, predict=FALSE){
tokenizer_words <- attributes(tokenizer[[1]])$names
text <- c()
if(predict == TRUE){
logits <- logits - 1 ## For prediction conversion only
}
for(i in logits){
if(i == 0){
text <- c(text, "<PAD>")
}else{
text <- c(text, tokenizer_words[i])
}
}
return(text)
}
# Testing to convert the first row back to text
# preprocess_x[[1]][1, ]
# preprocess_x[[1]]
logits_to_text(preprocess_x[[1]][1, ], x_tk)
library(keras)
library(tensorflow)
library(tokenizers)
library(dplyr)
# library(png)
library(reticulate)
# library(abind)
library(ramify)
library(stringr)
library(deepviz)
language <- "French"
language_code <- "fr"
file_name <- paste0("translation_", language_code, ".csv")
train <- read.csv(file_name, encoding="UTF-8", stringsAsFactors=FALSE)
colnames(train) <- c("English", language)
train
tokenize <- function(x){
tokenizer <- text_tokenizer(num_words = 1000000)
fit_text_tokenizer(tokenizer, x)
sequences <- texts_to_sequences(tokenizer, x)
return(c(sequences, tokenizer))
}
pad <- function(x, length=NULL){
return(pad_sequences(x, maxlen = length, padding = 'post'))
}
text_sentences = c('The quick brown fox jumps over the lazy dog .',
'By Jove , my quick study of lexicography won a prize .',
'This is a short sentence .')
token_index <- length(text_sentences) + 1
output <- tokenize(text_sentences)
text_tokenized <- output[1:length(text_sentences)]
# print(output)
# Finding out the integer allocation to each word
tk <- output[[token_index]]$word_index
# print(tk)
# print(length(tk))
# print(table(tk))
for(i in 1:length(text_sentences)){
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", text_sentences[i]))
print(paste0("Output: ", list(text_tokenized[[i]])))
cat("\n")
}
padded_text <- pad(text_tokenized)
for(i in 1:length(text_sentences)){
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", text_sentences[i]))
print(paste0("Output: ", list(text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(padded_text[i,])))
}
# n <- nrow(subset_train)
n <- 5
word_list <- list(train[, 1])[[1]][1:n]
# word_list
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
# if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
# cat("\n")
}
# n <- nrow(subset_train)
n <- 5
word_list <- list(train[, 2])[[1]][1:n]
new_output <- tokenize(word_list)
new_text_tokenized <- new_output[1:n]
new_padded_text <- pad(new_text_tokenized)
for(i in 1:n){
# if(i %% 100 != 0) next
print(paste0("Sequence in Text ", i, ":"))
print(paste0("Input: ", word_list[i]))
print(paste0("Output: ", list(new_text_tokenized[[i]])))
print(paste0("Output (Padded): ", list(new_padded_text[i,])))
}
preprocess_text <- function(x, y){
output_x <- tokenize(x)
output_y <- tokenize(y)
preprocess_x <- output_x[1:length(x)]; x_tk <- output_x[[length(x) + 1]]$word_index
preprocess_y <- output_y[1:length(y)]; y_tk <- output_y[[length(y) + 1]]$word_index
# print(preprocess_x)
preprocess_x <- pad(preprocess_x)
preprocess_y <- pad(preprocess_y)
# print(preprocess_x)
# Converting from a 2D matrix to a 3D tensor
# preprocess_x <- array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# preprocess_y <- array(preprocess_y[[1]], c(dim(preprocess_y[[1]])[1], dim(preprocess_y[[1]])[2], 1))
return(list(preprocess_x, preprocess_y, x_tk, y_tk))
}
train_x <- list(train[, 1])[[1]]
train_y <- list(train[, 2])[[1]]
# print(subset_train_x)
process_output <- preprocess_text(train_x, train_y)
# print(process_output[4],)
preprocess_x <- process_output[1]; preprocess_y <- process_output[2]; x_tk <- process_output[3]; y_tk <- process_output[4]
# print(preprocess_x[[1]])
# print(preprocess_y[[1]])
# Conversion back to list of words from tokenized word list
# attributes(x_tk[[1]])$names
# length(y_tk[[1]])
col_x <- dim(preprocess_x[[1]])[2]
col_y <- dim(preprocess_y[[1]])[2]
if(col_x >= col_y){
max_col <- col_x
}else{
max_col <- col_y
}
tmp_x <- pad(preprocess_x[[1]], max_col)
tmp_y <- pad(preprocess_y[[1]], max_col)
calculate_sparsity <- function(df_matrix){
zero_count <- 0
total_count <- nrow(df_matrix) * ncol(tmp_x)
for(i in 1:nrow(df_matrix)){
for(j in 1:ncol(df_matrix)){
if(df_matrix[i, j] == 0){
zero_count = zero_count + 1
}
}
}
zero_count/total_count
}
print(paste("The Sparsity of the matrix is: ", round(calculate_sparsity(tmp_x)*100, 2), "%"))
convert2tensor <- function(preprocess_data){
preprocess_data <- array(preprocess_data, c(dim(preprocess_data)[1], dim(preprocess_data)[2], 1))
return(preprocess_data)
}
# array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1))
# dim(array(preprocess_x[[1]], c(dim(preprocess_x[[1]])[1], dim(preprocess_x[[1]])[2], 1)))[2:3]
tensor_x <- convert2tensor(tmp_x)
dim(tensor_x)
tensor_x[1, , ]
tensor_y <- convert2tensor(tmp_y)
# tensor_y
logits_to_text <- function(logits, tokenizer, predict=FALSE){
tokenizer_words <- attributes(tokenizer[[1]])$names
text <- c()
if(predict == TRUE){
logits <- logits - 1 ## For prediction conversion only
}
for(i in logits){
if(i == 0){
text <- c(text, "<PAD>")
}else{
text <- c(text, tokenizer_words[i])
}
}
return(text)
}
# Testing to convert the first row back to text
# preprocess_x[[1]][1, ]
# preprocess_x[[1]]
logits_to_text(preprocess_x[[1]][1, ], x_tk)
=======
calculate_sparsity(tmp_x)
# Include your libraries here:
library(ggplot2)
library(tidymodels)
library(tidyr)
library(kernlab) # for svm functions
tidymodels_prefer()
source("classification_report.R") # function cr()
get2DIrisData <- function(){
# Get the iris data and project it down to two dimensions (for visualization)
newiris <- iris
# Convert the factor variable to integer
newiris$Species <- as.integer(unclass(newiris$Species))
# Convert the data frame to a matrix
irismatrix <- as.matrix(newiris)
# Remove the dimension names
dimnames(irismatrix) <- NULL
# Scale the data
features <- scale(irismatrix[,1:4])
# Append the labels column
irismatrix <- cbind(features,irismatrix[,5])
rowcount <- nrow(irismatrix)
set.seed(392) # So sample() always returns the same result (choose your own seed)
# We want an equal number of observations of each species in the training set
# We take advantage of the fact that there are exactly 50 flowers in each species
speciescount <- 50
speciestraincount <- round(speciescount*.8)
trainrows = unlist(map(1:3,function(speciesindex){
speciescount*(speciesindex-1)+
sort(sample(speciescount,speciestraincount))}))
trainset <- irismatrix[trainrows,]
testset <- irismatrix[-trainrows,]
numfeatures <- 4
x_trainset <- trainset[,1:numfeatures]
x_testset <- testset[,1:numfeatures]
pc <- prcomp(x_trainset,center=TRUE,scale.=FALSE,rank.=2)
rotationmatrix <- pc$rotation
traindata <- as.data.frame(pc$x)
# restore species names
speciesNames <- levels(iris$Species)
traindata$y <- speciesNames[trainset[,5]]
# Apply rotation to testset
testdata <- as.data.frame(x_testset %*% rotationmatrix)
testdata$y <- speciesNames[testset[,5]]
alldata <- list(traindata=traindata,testdata=testdata)
alldata
}
alldata <- get2DIrisData()
traindata <- alldata$traindata
testdata <- alldata$testdata
# Convert the outcome variable to a factor
traindata$y <- factor(traindata$y)
testdata$y <- factor(testdata$y)
head(traindata)
svm_rbf_model <- svm_rbf(cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
set.seed(55) # For repeatability
# Call vfold_cv() on the training set and request 10 folds
iris_folds <- vfold_cv(traindata, 10)
# Examine the result
#iris_folds
# Define a grid for the parameter search space.
# We create our own grid:
# a data frame with a column for each parameter (only one in this case)
costrange <- c(0.5,10)
costspan <- costrange[2]-costrange[1]
n <- 20
paramgrid <- as.data.frame(list(cost=(1:n)/n*costspan+costrange[1]))
# Remove earlier versions of the workflow, if any
rm(svm_wflow,tuned_wflow)
# Set up a workflow
svm_wflow <-
workflow() %>%  # This creates a workflow object
add_model(svm_rbf_model) # Every workflow must have a model
# There is no pre-processing required so the recipe just identifies the outcome variable
iris_recipe <- recipe(y ~ .,data=traindata)
# Add the recipe
svm_wflow <- svm_wflow %>% add_recipe(iris_recipe)
# Run model over the search grid
tuned_wflow <-  tune_grid(svm_wflow,
resamples=iris_folds,
grid=paramgrid)
# The autoplot() function creates a nice ggplot
autoplot(tuned_wflow) + theme(legend.position = "top")
# Include your libraries here:
library(ggplot2)
library(tidymodels)
library(tidyr)
library(kernlab) # for svm functions
tidymodels_prefer()
source("classification_report.R") # function cr()
get2DIrisData <- function(){
# Get the iris data and project it down to two dimensions (for visualization)
newiris <- iris
# Convert the factor variable to integer
newiris$Species <- as.integer(unclass(newiris$Species))
# Convert the data frame to a matrix
irismatrix <- as.matrix(newiris)
# Remove the dimension names
dimnames(irismatrix) <- NULL
# Scale the data
features <- scale(irismatrix[,1:4])
# Append the labels column
irismatrix <- cbind(features,irismatrix[,5])
rowcount <- nrow(irismatrix)
set.seed(392) # So sample() always returns the same result (choose your own seed)
# We want an equal number of observations of each species in the training set
# We take advantage of the fact that there are exactly 50 flowers in each species
speciescount <- 50
speciestraincount <- round(speciescount*.8)
trainrows = unlist(map(1:3,function(speciesindex){
speciescount*(speciesindex-1)+
sort(sample(speciescount,speciestraincount))}))
trainset <- irismatrix[trainrows,]
testset <- irismatrix[-trainrows,]
numfeatures <- 4
x_trainset <- trainset[,1:numfeatures]
x_testset <- testset[,1:numfeatures]
pc <- prcomp(x_trainset,center=TRUE,scale.=FALSE,rank.=2)
rotationmatrix <- pc$rotation
traindata <- as.data.frame(pc$x)
# restore species names
speciesNames <- levels(iris$Species)
traindata$y <- speciesNames[trainset[,5]]
# Apply rotation to testset
testdata <- as.data.frame(x_testset %*% rotationmatrix)
testdata$y <- speciesNames[testset[,5]]
alldata <- list(traindata=traindata,testdata=testdata)
alldata
}
alldata <- get2DIrisData()
traindata <- alldata$traindata
testdata <- alldata$testdata
# Convert the outcome variable to a factor
traindata$y <- factor(traindata$y)
testdata$y <- factor(testdata$y)
head(traindata)
svm_rbf_model <- svm_rbf(cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
set.seed(55) # For repeatability
# Call vfold_cv() on the training set and request 10 folds
iris_folds <- vfold_cv(traindata, v=10)
# Examine the result
iris_folds
# Define a grid for the parameter search space.
# We create our own grid:
# a data frame with a column for each parameter (only one in this case)
costrange <- c(0.5,10)
costspan <- costrange[2]-costrange[1]
n <- 20
paramgrid <- as.data.frame(list(cost=(1:n)/n*costspan+costrange[1]))
# Remove earlier versions of the workflow, if any
rm(svm_wflow,tuned_wflow)
# Set up a workflow
svm_wflow <-
workflow() %>%  # This creates a workflow object
add_model(svm_rbf_model) # Every workflow must have a model
# There is no pre-processing required so the recipe just identifies the outcome variable
iris_recipe <- recipe(y ~ .,data=traindata)
# Add the recipe
svm_wflow <- svm_wflow %>% add_recipe(iris_recipe)
# Run model over the search grid
tuned_wflow <-  tune_grid(svm_wflow,
resamples=iris_folds,
grid=paramgrid)
# The autoplot() function creates a nice ggplot
autoplot(tuned_wflow) + theme(legend.position = "top")
show_best(tuned_wflow,metric = "accuracy")
bestcost <- 3.825
svm_rbf_spec <- svm_rbf(cost =bestcost) %>%
set_mode("classification") %>%
set_engine("kernlab")
iris_wflow <-
workflow() %>%  # This creates a workflow object
add_model(svm_rbf_spec) %>%
add_recipe(iris_recipe)
#
iris_fit <- fit(iris_wflow,traindata)
# The object iris_fit now contains the fitted model
iris_fit
iris_fit_results <- bind_cols(
predict(iris_fit, traindata),
predict(iris_fit, traindata, type = "prob"),
) %>% mutate(y=traindata$y)
iris_fit_results
iriscm <- conf_mat(data=iris_fit_results ,truth=y,estimate=.pred_class)
print(iriscm)
iris_cr <- cr(as.matrix(iriscm$table))
print(iris_cr)
iristest_fit_results <- bind_cols(
predict(iris_fit, testdata),
predict(iris_fit, testdata, type = "prob"),
) %>% mutate(y=testdata$y)
iristestcm <- conf_mat(data=iris_fit_results ,truth=y,estimate=.pred_class)
print(iristestcm)
iristest_cr <- cr(as.matrix(iristestcm$table))
print(iristest_cr)
# Include your libraries here:
library(ggplot2)
library(tidymodels)
library(tidyr)
library(kernlab) # for svm functions
tidymodels_prefer()
source("classification_report.R") # function cr()
get2DIrisData <- function(){
# Get the iris data and project it down to two dimensions (for visualization)
newiris <- iris
# Convert the factor variable to integer
newiris$Species <- as.integer(unclass(newiris$Species))
# Convert the data frame to a matrix
irismatrix <- as.matrix(newiris)
# Remove the dimension names
dimnames(irismatrix) <- NULL
# Scale the data
features <- scale(irismatrix[,1:4])
# Append the labels column
irismatrix <- cbind(features,irismatrix[,5])
rowcount <- nrow(irismatrix)
set.seed(392) # So sample() always returns the same result (choose your own seed)
# We want an equal number of observations of each species in the training set
# We take advantage of the fact that there are exactly 50 flowers in each species
speciescount <- 50
speciestraincount <- round(speciescount*.8)
trainrows = unlist(map(1:3,function(speciesindex){
speciescount*(speciesindex-1)+
sort(sample(speciescount,speciestraincount))}))
trainset <- irismatrix[trainrows,]
testset <- irismatrix[-trainrows,]
numfeatures <- 4
x_trainset <- trainset[,1:numfeatures]
x_testset <- testset[,1:numfeatures]
pc <- prcomp(x_trainset,center=TRUE,scale.=FALSE,rank.=2)
rotationmatrix <- pc$rotation
traindata <- as.data.frame(pc$x)
# restore species names
speciesNames <- levels(iris$Species)
traindata$y <- speciesNames[trainset[,5]]
# Apply rotation to testset
testdata <- as.data.frame(x_testset %*% rotationmatrix)
testdata$y <- speciesNames[testset[,5]]
alldata <- list(traindata=traindata,testdata=testdata)
alldata
}
alldata <- get2DIrisData()
traindata <- alldata$traindata
testdata <- alldata$testdata
# Convert the outcome variable to a factor
traindata$y <- factor(traindata$y)
testdata$y <- factor(testdata$y)
head(traindata)
svm_rbf_model <- svm_rbf(cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
set.seed(55) # For repeatability
# Call vfold_cv() on the training set and request 10 folds
iris_folds <- vfold_cv(traindata, 10)
# Examine the result
#iris_folds
# Define a grid for the parameter search space.
# We create our own grid:
# a data frame with a column for each parameter (only one in this case)
costrange <- c(0.5,10)
costspan <- costrange[2]-costrange[1]
n <- 20
paramgrid <- as.data.frame(list(cost=(1:n)/n*costspan+costrange[1]))
# Remove earlier versions of the workflow, if any
rm(svm_wflow,tuned_wflow)
# Set up a workflow
svm_wflow <-
workflow() %>%  # This creates a workflow object
add_model(svm_rbf_model) # Every workflow must have a model
# There is no pre-processing required so the recipe just identifies the outcome variable
iris_recipe <- recipe(y ~ .,data=traindata)
# Add the recipe
svm_wflow <- svm_wflow %>% add_recipe(iris_recipe)
# Run model over the search grid
tuned_wflow <-  tune_grid(svm_wflow,
resamples=iris_folds,
grid=paramgrid)
# The autoplot() function creates a nice ggplot
autoplot(tuned_wflow) + theme(legend.position = "top")
show_best(tuned_wflow, "accuracy")
bestcost <- 4.3
svm_rbf_spec <- svm_rbf(cost =bestcost) %>%
set_mode("classification") %>%
set_engine("kernlab")
iris_wflow <-
workflow() %>%  # This creates a workflow object
add_model(svm_rbf_spec) %>%
add_recipe(iris_recipe)
#
iris_fit <- fit(iris_wflow,traindata)
# The object iris_fit now contains the fitted model
iris_fit
iris_fit_results <- bind_cols(
predict(iris_fit, traindata),
predict(iris_fit, traindata, type = "prob"),
) %>% mutate(y=traindata$y)
iriscm <- conf_mat(data=iris_fit_results ,truth=y,estimate=.pred_class)
print(iriscm)
iris_cr <- cr(iriscm$table)
print(iris_cr)
iristest_fit_results <- bind_cols(
predict(iris_fit, testdata),
predict(iris_fit, testdata, type = "prob"),
) %>% mutate(y=testdata$y)
iristestcm <- conf_mat(data=iristest_fit_results ,truth=y,estimate=.pred_class)
print(iristestcm)
iristest_cr <- cr(iristestcm$table)
print(iristest_cr)
# Include your libraries here:
library(ggplot2)
library(tidymodels)
library(tidyr)
library(kernlab) # for svm functions
tidymodels_prefer()
source("classification_report.R") # function cr()
get2DIrisData <- function(){
# Get the iris data and project it down to two dimensions (for visualization)
newiris <- iris
# Convert the factor variable to integer
newiris$Species <- as.integer(unclass(newiris$Species))
# Convert the data frame to a matrix
irismatrix <- as.matrix(newiris)
# Remove the dimension names
dimnames(irismatrix) <- NULL
# Scale the data
features <- scale(irismatrix[,1:4])
# Append the labels column
irismatrix <- cbind(features,irismatrix[,5])
rowcount <- nrow(irismatrix)
set.seed(392) # So sample() always returns the same result (choose your own seed)
# We want an equal number of observations of each species in the training set
# We take advantage of the fact that there are exactly 50 flowers in each species
speciescount <- 50
speciestraincount <- round(speciescount*.8)
trainrows = unlist(map(1:3,function(speciesindex){
speciescount*(speciesindex-1)+
sort(sample(speciescount,speciestraincount))}))
trainset <- irismatrix[trainrows,]
testset <- irismatrix[-trainrows,]
numfeatures <- 4
x_trainset <- trainset[,1:numfeatures]
x_testset <- testset[,1:numfeatures]
pc <- prcomp(x_trainset,center=TRUE,scale.=FALSE,rank.=2)
rotationmatrix <- pc$rotation
traindata <- as.data.frame(pc$x)
# restore species names
speciesNames <- levels(iris$Species)
traindata$y <- speciesNames[trainset[,5]]
# Apply rotation to testset
testdata <- as.data.frame(x_testset %*% rotationmatrix)
testdata$y <- speciesNames[testset[,5]]
alldata <- list(traindata=traindata,testdata=testdata)
alldata
}
alldata <- get2DIrisData()
traindata <- alldata$traindata
testdata <- alldata$testdata
# Convert the outcome variable to a factor
traindata$y <- factor(traindata$y)
testdata$y <- factor(testdata$y)
head(traindata)
svm_rbf_model <- svm_rbf(cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
set.seed(55) # For repeatability
# Call vfold_cv() on the training set and request 10 folds
iris_folds <- vfold_cv(traindata, v=10)
# Examine the result
iris_folds
# Define a grid for the parameter search space.
# We create our own grid:
# a data frame with a column for each parameter (only one in this case)
costrange <- c(0.5,10)
costspan <- costrange[2]-costrange[1]
n <- 20
paramgrid <- as.data.frame(list(cost=(1:n)/n*costspan+costrange[1]))
# Remove earlier versions of the workflow, if any
rm(svm_wflow,tuned_wflow)
# Set up a workflow
svm_wflow <-
workflow() %>%  # This creates a workflow object
add_model(svm_rbf_model) # Every workflow must have a model
# There is no pre-processing required so the recipe just identifies the outcome variable
iris_recipe <- recipe(y ~ .,data=traindata)
# Add the recipe
svm_wflow <- svm_wflow %>% add_recipe(iris_recipe)
# Run model over the search grid
tuned_wflow <-  tune_grid(svm_wflow,
resamples=iris_folds,
grid=paramgrid)
# The autoplot() function creates a nice ggplot
autoplot(tuned_wflow) + theme(legend.position = "top")
show_best(tuned_wflow,metric = "accuracy")
bestcost <- 3.825
svm_rbf_spec <- svm_rbf(cost =bestcost) %>%
set_mode("classification") %>%
set_engine("kernlab")
iris_wflow <-
workflow() %>%  # This creates a workflow object
add_model(svm_rbf_spec) %>%
add_recipe(iris_recipe)
#
iris_fit <- fit(iris_wflow,traindata)
# The object iris_fit now contains the fitted model
iris_fit
iris_fit_results <- bind_cols(
predict(iris_fit, traindata),
predict(iris_fit, traindata, type = "prob"),
) %>% mutate(y=traindata$y)
iris_fit_results
iriscm <- conf_mat(data=iris_fit_results ,truth=y,estimate=.pred_class)
print(iriscm)
iris_cr <- cr(as.matrix(iriscm$table))
print(iris_cr)
iristest_fit_results <- bind_cols(
predict(iris_fit, testdata),
predict(iris_fit, testdata, type = "prob"),
) %>% mutate(y=testdata$y)
iristestcm <- conf_mat(data=iristest_fit_results ,truth=y,estimate=.pred_class)
print(iristestcm)
iristest_cr <- cr(as.matrix(iristestcm$table))
print(iristest_cr)
pred_translation <- function(i){
predict_output <- model_RNN %>% predict(matrix(tensor_x[i, ,], nrow=1))
predict_output <- argmax(predict_output, FALSE)
converted_text <- logits_to_text(predict_output, y_tk, predict = TRUE)
converted_text[converted_text == "<PAD>"] <- ""
converted_text <- trimws(paste(converted_text, collapse = " "))
print(paste("Input sentence:", train_x[i]))
print(paste("Intended Output Sentence:", train_y[i]))
print(paste("Predicted Output Sentence:", converted_text))
}
## `i` represents the index within the training set.
pred_translation(5)
>>>>>>> Stashed changes
