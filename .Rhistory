dprev_s = t(W) %*% dmulw
for (i in (t-1):max(-1, (t-bptt_truncate-1))){
ds = dsv + dprev_s
dadd = add * (1 - add) * ds
dmulw = dadd * rep(1, length(mulw))
dmulu = dadd * rep(1, length(mulu))
dW_i = W %*% layers[t,2,]
dprev_s = t(W) %*% dmulw
new_input = rep(0, length(x))
new_input[t] = x[t]
dU_i = U %*% new_input
dx = t(U) %*% dmulu
dU_t = dU_t + dU_i
dW_t = dW_t + dW_i
}
dV = dV + dV_t
dU = dU + dU_t
dW = dW + dW_t
if (max(dU) > max_clip_value){
dU[dU > max_clip_value] = max_clip_value
}
if (max(dV) > max_clip_value){
dV[dV > max_clip_value] = max_clip_value
}
if (max(dW) > max_clip_value){
dW[dW > max_clip_value] = max_clip_value
}
if (min(dU) < min_clip_value){
dU[dU < min_clip_value] = min_clip_value
}
if (min(dV) < min_clip_value){
dV[dV < min_clip_value] = min_clip_value
}
if (min(dW) < min_clip_value){
dW[dW < min_clip_value] = min_clip_value
}
}
U = U - learning_rate * dU
V = V - learning_rate * dV
W = W - learning_rate * dW
}
}
dU_t
dim(dU_t)
dim(dU_i)
dim(rep(dU_t,50))
rep(dU_t,50)
rowAdd
dU_t + dU_t
dU_t + dU_i
dU_t[,] + dU_i
dU_i
rep(dU_i, c(,50))
rep(dU_i, 50)
rbind(dU_i, dU_i[rep(1, 50),])
?apply
apply(dU_t, 1, function(x) x + dU_i)
dim(apply(dU_t, 1, function(x) x + dU_i))
apply(dU_t, 1, function(x) print(x))
apply(dU_t, 1)
apply(dU_t, 1, sum)
apply(dU_t, 2, function(x)x+dU_i)
library(dplyr)
library(tidyverse)
sine <- as.matrix(sin(1:200))
plot(sine[1:50],xlab="x",ylab="y = sin(x)")
lines(1:50, sine[1:50])
X = NULL
Y = NULL
sequence_length = 50
num_records = length(sine) - sequence_length
for (i in 1:(num_records-50)){
X = rbind(X, sine[i:(i+sequence_length-1)])
Y = rbind(Y, sine[i+sequence_length-1])
}
X_val = NULL
Y_val = NULL
for (i in (num_records-49):(num_records)){
X_val = rbind(X_val, sine[i:(i+sequence_length-1)])
Y_val = rbind(Y_val, sine[i+sequence_length-1])
}
learning_rate = 0.001
epochs = 25
hidden_dim = 100
output_dim = 1
bptt_truncate = 5
min_clip_value = -10
max_clip_value = 10
set.seed(123)
U = matrix(runif(hidden_dim * sequence_length), nrow = hidden_dim, ncol = sequence_length)
W = matrix(runif(hidden_dim * hidden_dim), nrow = hidden_dim, ncol = hidden_dim)
V = matrix(runif(output_dim * hidden_dim), nrow = output_dim, ncol = hidden_dim)
sigmoid <- function(x){
1 / (1 + exp(-x))
}
for (epoch in 1:epochs){
loss = 0
for (i in 1:dim(Y)[1]){
x <- X[i,]
y <- Y[i]
prev_s <- rep(0, hidden_dim) # Initializing the previous activation as a vector of 0s.
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
prev_s <- s
}
loss_per_record <- ((y - mulv)^2)/2
loss = loss + loss_per_record
}
loss = loss/length(y)
val_loss = 0
for (i in 1:dim(Y_val)[1]){
x <- X_val[i,]
y <- Y_val[i]
prev_s <- rep(0, hidden_dim)
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
prev_s <- s
}
loss_per_record <- ((y - mulv)^2)/2
val_loss = val_loss + loss_per_record
}
val_loss <- val_loss/length(y)
print(paste0("Epoch: ", epoch, ", Loss: ", loss, ", Val Loss: ", val_loss))
for (i in 1:dim(Y)[1]){
x <- X[i,]
y <- Y[i]
layers <- array(rep(NaN, sequence_length * hidden_dim * 2), c(sequence_length, 2, hidden_dim)) # Our layers matrix for the given sequence_length = 50, hidden_dim = 100, is 50 x 2 x 100. Hence, we initialize an array with NaN on those dimensions here.
prev_s <- rep(0, hidden_dim)
dU <- 0 * U
dV <- 0 * V
dW <- 0 * W
dU_t <- 0 * U
dV_t <- 0 * V
dW_t <- 0 * W
dU_i <- 0 * U
dW_i <- 0 * W
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
layers[t,1,] <- s
layers[t,2,] <- prev_s
prev_s <- s
}
dmulv <- mulv - y
for (t in 1:sequence_length){
dV_t <- dmulv %*% t(layers[t,1,])
dsv <- t(V) %*% dmulv
ds = dsv
dadd = add * (1 - add) * ds
dmulw = dadd * rep(1, length(mulw))
dprev_s = t(W) %*% dmulw
for (i in (t-1):max(-1, (t-bptt_truncate-1))){
ds = dsv + dprev_s
dadd = add * (1 - add) * ds
dmulw = dadd * rep(1, length(mulw))
dmulu = dadd * rep(1, length(mulu))
dW_i = W %*% layers[t,2,]
dprev_s = t(W) %*% dmulw
new_input = rep(0, length(x))
new_input[t] = x[t]
dU_i = U %*% new_input
dx = t(U) %*% dmulu
dU_t = apply(dU_t, 2, function(x)x+dU_i)
dW_t = apply(dW_t, 2, function(x)x+dW_i)
}
dV = dV + dV_t
dU = dU + dU_t
dW = dW + dW_t
if (max(dU) > max_clip_value){
dU[dU > max_clip_value] = max_clip_value
}
if (max(dV) > max_clip_value){
dV[dV > max_clip_value] = max_clip_value
}
if (max(dW) > max_clip_value){
dW[dW > max_clip_value] = max_clip_value
}
if (min(dU) < min_clip_value){
dU[dU < min_clip_value] = min_clip_value
}
if (min(dV) < min_clip_value){
dV[dV < min_clip_value] = min_clip_value
}
if (min(dW) < min_clip_value){
dW[dW < min_clip_value] = min_clip_value
}
}
U = U - learning_rate * dU
V = V - learning_rate * dV
W = W - learning_rate * dW
}
}
library(profvis)
install.packages("profvis")
library(profvis)
library(dplyr)
library(tidyverse)
library(profvis)
sine <- as.matrix(sin(1:200))
plot(sine[1:50],xlab="x",ylab="y = sin(x)")
lines(1:50, sine[1:50])
X = NULL
Y = NULL
sequence_length = 50
num_records = length(sine) - sequence_length
for (i in 1:(num_records-50)){
X = rbind(X, sine[i:(i+sequence_length-1)])
Y = rbind(Y, sine[i+sequence_length-1])
}
X_val = NULL
Y_val = NULL
for (i in (num_records-49):(num_records)){
X_val = rbind(X_val, sine[i:(i+sequence_length-1)])
Y_val = rbind(Y_val, sine[i+sequence_length-1])
}
learning_rate = 0.001
epochs = 25
hidden_dim = 100
output_dim = 1
bptt_truncate = 5
min_clip_value = -10
max_clip_value = 10
set.seed(123)
U = matrix(runif(hidden_dim * sequence_length), nrow = hidden_dim, ncol = sequence_length)
W = matrix(runif(hidden_dim * hidden_dim), nrow = hidden_dim, ncol = hidden_dim)
V = matrix(runif(output_dim * hidden_dim), nrow = output_dim, ncol = hidden_dim)
sigmoid <- function(x){
1 / (1 + exp(-x))
}
profvis({
for (epoch in 1:epochs){
loss = 0
for (i in 1:dim(Y)[1]){
x <- X[i,]
y <- Y[i]
prev_s <- rep(0, hidden_dim) # Initializing the previous activation as a vector of 0s.
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
prev_s <- s
}
loss_per_record <- ((y - mulv)^2)/2
loss = loss + loss_per_record
}
loss = loss/length(y)
val_loss = 0
for (i in 1:dim(Y_val)[1]){
x <- X_val[i,]
y <- Y_val[i]
prev_s <- rep(0, hidden_dim)
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
prev_s <- s
}
loss_per_record <- ((y - mulv)^2)/2
val_loss = val_loss + loss_per_record
}
val_loss <- val_loss/length(y)
print(paste0("Epoch: ", epoch, ", Loss: ", loss, ", Val Loss: ", val_loss))
for (i in 1:dim(Y)[1]){
x <- X[i,]
y <- Y[i]
layers <- array(rep(NaN, sequence_length * hidden_dim * 2), c(sequence_length, 2, hidden_dim)) # Our layers matrix for the given sequence_length = 50, hidden_dim = 100, is 50 x 2 x 100. Hence, we initialize an array with NaN on those dimensions here.
prev_s <- rep(0, hidden_dim)
dU <- 0 * U
dV <- 0 * V
dW <- 0 * W
dU_t <- 0 * U
dV_t <- 0 * V
dW_t <- 0 * W
dU_i <- 0 * U
dW_i <- 0 * W
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
layers[t,1,] <- s
layers[t,2,] <- prev_s
prev_s <- s
}
dmulv <- mulv - y
for (t in 1:sequence_length){
dV_t <- dmulv %*% t(layers[t,1,])
dsv <- t(V) %*% dmulv
ds = dsv
dadd = add * (1 - add) * ds
dmulw = dadd * rep(1, length(mulw))
dprev_s = t(W) %*% dmulw
for (i in (t-1):max(-1, (t-bptt_truncate-1))){
ds = dsv + dprev_s
dadd = add * (1 - add) * ds
dmulw = dadd * rep(1, length(mulw))
dmulu = dadd * rep(1, length(mulu))
dW_i = W %*% layers[t,2,]
dprev_s = t(W) %*% dmulw
new_input = rep(0, length(x))
new_input[t] = x[t]
dU_i = U %*% new_input
dx = t(U) %*% dmulu
dU_t = apply(dU_t, 2, function(x)x+dU_i) # Adding dU_i (100 x 1) to all columns of dU_t (100 x 50)
dW_t = apply(dW_t, 2, function(x)x+dW_i)
}
dV = dV + dV_t
dU = dU + dU_t
dW = dW + dW_t
if (max(dU) > max_clip_value){
dU[dU > max_clip_value] = max_clip_value
}
if (max(dV) > max_clip_value){
dV[dV > max_clip_value] = max_clip_value
}
if (max(dW) > max_clip_value){
dW[dW > max_clip_value] = max_clip_value
}
if (min(dU) < min_clip_value){
dU[dU < min_clip_value] = min_clip_value
}
if (min(dV) < min_clip_value){
dV[dV < min_clip_value] = min_clip_value
}
if (min(dW) < min_clip_value){
dW[dW < min_clip_value] = min_clip_value
}
}
U = U - learning_rate * dU
V = V - learning_rate * dV
W = W - learning_rate * dW
}
}
})
library(dplyr)
library(tidyverse)
library(profvis)
sine <- as.matrix(sin(1:200))
plot(sine[1:50],xlab="x",ylab="y = sin(x)")
lines(1:50, sine[1:50])
X = NULL
Y = NULL
sequence_length = 50
num_records = length(sine) - sequence_length
for (i in 1:(num_records-50)){
X = rbind(X, sine[i:(i+sequence_length-1)])
Y = rbind(Y, sine[i+sequence_length-1])
}
X_val = NULL
Y_val = NULL
for (i in (num_records-49):(num_records)){
X_val = rbind(X_val, sine[i:(i+sequence_length-1)])
Y_val = rbind(Y_val, sine[i+sequence_length-1])
}
learning_rate = 0.001
epochs = 25
hidden_dim = 100
output_dim = 1
bptt_truncate = 5
min_clip_value = -10
max_clip_value = 10
set.seed(123)
U = matrix(runif(hidden_dim * sequence_length), nrow = hidden_dim, ncol = sequence_length)
W = matrix(runif(hidden_dim * hidden_dim), nrow = hidden_dim, ncol = hidden_dim)
V = matrix(runif(output_dim * hidden_dim), nrow = output_dim, ncol = hidden_dim)
sigmoid <- function(x){
1 / (1 + exp(-x))
}
profvis({
for (epoch in 1:epochs){
loss = 0
for (i in 1:dim(Y)[1]){
x <- X[i,]
y <- Y[i]
prev_s <- rep(0, hidden_dim) # Initializing the previous activation as a vector of 0s.
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
prev_s <- s
}
loss_per_record <- ((y - mulv)^2)/2
loss = loss + loss_per_record
}
loss = loss/length(y)
val_loss = 0
for (i in 1:dim(Y_val)[1]){
x <- X_val[i,]
y <- Y_val[i]
prev_s <- rep(0, hidden_dim)
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
prev_s <- s
}
loss_per_record <- ((y - mulv)^2)/2
val_loss = val_loss + loss_per_record
}
val_loss <- val_loss/length(y)
print(paste0("Epoch: ", epoch, ", Loss: ", loss, ", Val Loss: ", val_loss))
for (i in 1:dim(Y)[1]){
x <- X[i,]
y <- Y[i]
layers <- array(rep(NaN, sequence_length * hidden_dim * 2), c(sequence_length, 2, hidden_dim)) # Our layers matrix for the given sequence_length = 50, hidden_dim = 100, is 50 x 2 x 100. Hence, we initialize an array with NaN on those dimensions here.
prev_s <- rep(0, hidden_dim)
dU <- 0 * U
dV <- 0 * V
dW <- 0 * W
dU_t <- 0 * U
dV_t <- 0 * V
dW_t <- 0 * W
dU_i <- 0 * U
dW_i <- 0 * W
for (t in 1:sequence_length){
new_input <- rep(0, length(x))
new_input[t] <- x[t]
mulu <- U %*% new_input
mulw <- W %*% prev_s
add <- mulw + mulu
s = sigmoid(add)
mulv <- V %*% s
layers[t,1,] <- s
layers[t,2,] <- prev_s
prev_s <- s
}
dmulv <- mulv - y
for (t in 1:sequence_length){
dV_t <- dmulv %*% t(layers[t,1,])
dsv <- t(V) %*% dmulv
ds = dsv
dadd = add * (1 - add) * ds
dmulw = dadd * rep(1, length(mulw))
dprev_s = t(W) %*% dmulw
for (i in (t-1):max(-1, (t-bptt_truncate-1))){
ds = dsv + dprev_s
dadd = add * (1 - add) * ds
dmulw = dadd * rep(1, length(mulw))
dmulu = dadd * rep(1, length(mulu))
dW_i = W %*% layers[t,2,]
dprev_s = t(W) %*% dmulw
new_input = rep(0, length(x))
new_input[t] = x[t]
dU_i = U %*% new_input
dx = t(U) %*% dmulu
dU_t = apply(dU_t, 2, function(x)x+dU_i) # Adding dU_i (100 x 1) to all columns of dU_t (100 x 50)
dW_t = apply(dW_t, 2, function(x)x+dW_i)
}
dV = dV + dV_t
dU = dU + dU_t
dW = dW + dW_t
if (max(dU) > max_clip_value){
dU[dU > max_clip_value] = max_clip_value
}
if (max(dV) > max_clip_value){
dV[dV > max_clip_value] = max_clip_value
}
if (max(dW) > max_clip_value){
dW[dW > max_clip_value] = max_clip_value
}
if (min(dU) < min_clip_value){
dU[dU < min_clip_value] = min_clip_value
}
if (min(dV) < min_clip_value){
dV[dV < min_clip_value] = min_clip_value
}
if (min(dW) < min_clip_value){
dW[dW < min_clip_value] = min_clip_value
}
}
U = U - learning_rate * dU
V = V - learning_rate * dV
W = W - learning_rate * dW
}
}
})
